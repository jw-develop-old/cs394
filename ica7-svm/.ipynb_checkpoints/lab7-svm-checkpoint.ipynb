{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Activity 7: Support Vector Machines\n",
    "\n",
    "The goal of this lab activity is to demonstrate the use of support vector machines for classification using the tools available in `scikit-learn`.\n",
    "\n",
    "This activity is adapted from M&uuml;ller and Guido, *Introduction to Machine Learning with Python*, pg 58-68 and 94-105. \n",
    "As in certain previous labs, the code is drawn from what is available from the GitHub\n",
    "repositories supporting \n",
    "[the M&uuml;ller and Guido book](https://github.com/amueller/mglearn/tree/master/mglearn).\n",
    "\n",
    "Also, while I have your attention, I'd like to remind you all that the syllabus policy\n",
    "\n",
    ">  Please keep all\n",
    "laptops, tablets, phones, etc, silenced and put away. If you absolutely need to check your phone\n",
    "for something, please discreetly step out in to the hall.\n",
    "\n",
    "applies in lab as well as in lecture. \n",
    "Keep those phones put away.\n",
    "Check out this article sometime: [Your Smartphone Reduces Your Brainpower, Even if It's Just Sitting there](https://getpocket.com/explore/item/your-smartphone-reduces-your-brainpower-even-if-it-s-just-sitting-there)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear models for classification\n",
    "\n",
    "The general context for using support vector machines is classifying data using linear formulas that discriminate among classes. As we saw in class on Monday, we separate data points in different classes using a hyperplane defined by\n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "...with points classified by whether they give a positive or negative value for $\\mathbf{w}\\cdot \\mathbf{x} + b$. \n",
    "There are several approaches to classification based on hyperplanes, not just SVMs. \n",
    "They differ from each other on how to find an appropriate hyperplane based on the data, how to tolerate\n",
    "(or otherwise deal with) misclassifications, etc.\n",
    "\n",
    "One point of comparison for support vector machines is *logistic regression*, so called because it uses the logistic sigmoid function (also note that we are here applying it to classification, not regression).\n",
    "Before getting into details of either approaches, consider this example of applying them to a training data set using their `scikit-learn` implementation with default parameters. (If you get deprecation warnings, don't worry about them.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/lib64/python3.4/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/lib64/python3.4/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f689d6d2c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `C` in `LinearSVC` is for *classifier*. The data is not linearly separable, and although the lines drawn by `LinearSVC` and `LogisticRegression` aren't too far off, you can see that they do draw different lines.\n",
    "(I would like to have the actual support vectors appear in the SVM plot, but `LinearSVC` doesn't store the\n",
    "support vectors themselves, just the hyperplane.)\n",
    "\n",
    "In class last time we talked about how a soft margin classification is parameterized by `C`\n",
    "(which the `scikit-learn` documentation calls the *penalty parameter*). \n",
    "Consider the effect on the classification by these various settings of that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from mglearn.plot_helpers import discrete_scatter\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# a carefully hand-designed dataset \n",
    "y[7] = 0\n",
    "y[27] = 0\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "for ax, C in zip(axes, [1e-2, 10, 1e3]):\n",
    "    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "\n",
    "    svm = LinearSVC(C=C, tol=0.00001, dual=False).fit(X, y)\n",
    "    w = svm.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(6, 13)\n",
    "    yy = a * xx - (svm.intercept_[0]) / w[1]\n",
    "    ax.plot(xx, yy, c='k')\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(\"C = %f\" % C)\n",
    "axes[0].legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? \n",
    "How many points are misclassified, and how far off are they?\n",
    "Which displays evidence of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiclass classification\n",
    "\n",
    "Our discussion in class assumed binary classification, and in fact a straight-up\n",
    "linear approach inherently applies only to two classes.\n",
    "There are still ways to extend the idea to more than two classes.\n",
    "One way is to break the problem up into several binary classification problems, one\n",
    "for each class. \n",
    "The idea is that each of several classifiers can distinguish one class from all the others,\n",
    "called *one-vs-rest*.\n",
    "For a new data point, each classifier tests that point and identifies it as either in or out of\n",
    "the class. \n",
    "\n",
    "Ideally, exactly one classifier accepts that new point.\n",
    "If no classifier identifies the point as being in-class, then the point will have to be tagged\n",
    "as whatever class it was closest to.\n",
    "If more than one classifier accepts the point, then the tie will have to be broken\n",
    "by some measurement of how confident the classifiers are.\n",
    "\n",
    "Here we try this out with three classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes are linearly separable, so they should be easy to classify. \n",
    "Let's fit an SVM classifier to them. A `LinearSVC` has an array of coefficients \n",
    "($\\mathbf{w}$, since they are analogous to what we call *weights* in other contexts)\n",
    "and an intercept ($b$, analogous to the *bias*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(str(linear_svm.coef_))\n",
    "print(str(linear_svm.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand what these numbers mean and why the arrays have the shape that they do.\n",
    "Now we plot the lines with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three us-vs-them lines. \n",
    "There are three regions where there is no ambiguity about how a new data point would be classified,\n",
    "and the training data are all nicely in an appropriate region.\n",
    "There are also three overlap regions, and a \"no man's land\" in the middle. \n",
    "We can visualize how the decisions would go in these cases by shading the \n",
    "plot by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protocol for disambiguating points that land in overlap regions\n",
    "or no-man's land is straightforward: find the closest boundary line.\n",
    "How well does this perform?\n",
    "Go back to the beginning of this section and revise the code so that it generates a\n",
    "larger set of data (the `n_samples` parameter to `make_blobs`).\n",
    "Split that into training and test sets, and test the accuracy.\n",
    "\n",
    "(Remember that there are library functions that make this easy such as\n",
    "`sklearn.model_selection.train_test_split`.\n",
    "Like other classifiers, `LinearSVC` has a `score` method to compute accuracy on\n",
    "a test set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVMs and non-linearly-separable data\n",
    "\n",
    "The SVM classifiers we've looked at so far are called *linear* in scikit learn because they are not kernelized, and `LinearSVC` does not directly support using kernels.\n",
    "If the data isn't even close to being linearly separable, then there is little one can do \n",
    "using only linear models.\n",
    "Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fitting a linear SVM to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we talked about adding dimensionality to a dataset to make the data separable.\n",
    "Of course this data is separable, just not *linearly* separable.\n",
    "It turns out that in this example, squaring \"feature 1\" sharply discriminates the blues\n",
    "(high absolute value for feature 1) from the the oranges (low absolute value for feature 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# plot first all the points with y==0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can make a plane that slices the data, putting the orange on one side and the blue on the other. \n",
    "Here we train an SVM on the new features. Note that this is still using `LinearSVC`, because we transformed the data ourselves and are now applying a linear SVM to that transformed data---that's different from using an SVM classifier that itself supports kernelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take that classifier and apply it to the original data set.\n",
    "We can see how the plane is projected on the original feature space by coloring the background. \n",
    "This shows us the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kernelized SVMs\n",
    "\n",
    "The class `sklearn.svm.SVC` is parameterized by choice of kernel.\n",
    "It also differs from `LinearSVC` in that it does not store the coefficients and\n",
    "intercept for a discriminating line but rather the support vectors themselves---that is,\n",
    "it stores the vectors close to the boundary and which therefore define or *support*\n",
    "the boundary for classification.\n",
    "See [the documentation for SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "and compare it with [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).\n",
    "\n",
    "Predictions for new datapoints are made by comparing the distance to the support vectors of \n",
    "the various classes.\n",
    "The kernel function is used as a distance measure.\n",
    "A commonly used kernel is the *Gaussian kernel*, which \n",
    "is somehow derived from the Gaussian distribution.\n",
    "(I've scoured many sources on Kernelized SVMs, but they're all pretty short on \n",
    "intutition behind the Gaussian kernel. \n",
    "They tend to describe it as \"widely-used\", or \"one of the first kernels studied\"\n",
    "or \"produces good results\".)\n",
    "The formula is\n",
    "\n",
    "$$\n",
    "k_\\mbox{rbf}(x_1, x_2) = e^{-\\gamma || x_1 - x_2 ||^2}\n",
    "$$\n",
    "\n",
    "The double bars indicate the L2 norm (Euclidean distance). \n",
    "The parameter $\\gamma = \\frac{1}{2 \\sigma^2}$ stands in for the standard deviation\n",
    "and is used to control the width of the bell-curve. \n",
    "The subscript *rbf* is because this is also known as the *radial basis function*\n",
    "(improperly so, as far as I can tell---this is, in fact, only one kind of radial basis function,\n",
    "but that's another story).\n",
    "\n",
    "Let's observe this in practice. The support vectors themselves are highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()                                                                  \n",
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "# plot support vectors\n",
    "sv = svm.support_vectors_\n",
    "# class labels of support vectors are given by the sign of the dual coefficients\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundary is non-linear.\n",
    "Play around with the parameters in the above code (mainly `C` and `gamma`) and see what happens.\n",
    "Then try the following code which varies the parameters more systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "        \n",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
    "                  ncol=4, loc=(.9, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize what you can observe as $\\gamma$ increases (left to right) and as `C` increases (top to bottom).\n",
    "\n",
    "One thing I have not figured out is how the support vectors themselves are determined. \n",
    "In class Monday we talked about using linear SVMs for binary classification on linearly separable\n",
    "data, and in that case each class had one support vector, and the two support vectors (one from each class)\n",
    "were used to define the hyperplane. \n",
    "But in these cases it appears classes have many support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVMs and the data\n",
    "\n",
    "Now let's try this on some real data.\n",
    "The following code will load the breast cancer data set and train an SVM using \n",
    "the default parameters (which you can look up in \n",
    "[the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "cancer = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty low score on the test set.\n",
    "The high score on the training set looks like overfitting.\n",
    "Can you do better by adjusting the parameters?\n",
    "\n",
    "Of course in real life we don't want to fiddle with the parameters blindly.\n",
    "Let's see if looking at the data tells us something.\n",
    "The following code makes a \"min-max\" plot of the features in this data set.\n",
    "Each bar shows the range of values for a particular feature. \n",
    "Note that the vertical axis is on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(X_train, manage_xticks=False)\n",
    "plt.yscale(\"symlog\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this shows us is that different features are on very different scales, and some\n",
    "have large ranges, others small. \n",
    "The differences are in orders of magnitude.\n",
    "For good performance, SVMs need the features to be similarly scaled.\n",
    "\n",
    "We can help by preprocessing (rescaling) the data. \n",
    "Let's rescale each feature so that the values range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum value per feature on the training set\n",
    "min_on_training = X_train.min(axis=0)\n",
    "# Compute the range of each feature (max - min) on the training set\n",
    "range_on_training = (X_train - min_on_training).max(axis=0)\n",
    "\n",
    "# subtract the min, divide by range\n",
    "# afterward, min=0 and max=1 for each feature\n",
    "X_train_scaled = (X_train - min_on_training) / range_on_training\n",
    "\n",
    "# Sanity check\n",
    "print(\"Minimum for each feature\\n\", X_train_scaled.min(axis=0))\n",
    "print(\"Maximum for each feature\\n\", X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use THE SAME transformation on the test set,\n",
    "# using min and range of the training set. See Chapter 3 (unsupervised learning) for details.\n",
    "X_test_scaled = (X_test - min_on_training) / range_on_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "        svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's a lot better. But now we aren't getting the training set perfectly. \n",
    "Let's increase `C`. Remember, larger `C` means fewer misclassifications on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1000)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada. If time permits, try this on other data sets (digits, iris, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

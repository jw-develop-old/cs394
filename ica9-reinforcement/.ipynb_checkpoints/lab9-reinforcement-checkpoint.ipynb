{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Activity 9: Reinforcement learning\n",
    "\n",
    "The goal of this activity is to explore the concepts of reinforcement learning\n",
    "using the OpenAI Gym package.\n",
    "\n",
    "This activity is adapted from  Aur&eacute;lien G&eacute;ron, *Hands-On Machine Learning with Scikit-Learn and TensorFlow*, 2017, Chapter 16. (The code can be retreived from [the book's github page](https://github.com/ageron/handson-ml). \n",
    "However, the code available there differs from what appears in the book in several places, and \n",
    "in some cases to revert back to version in the book.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of concepts\n",
    "\n",
    "G&eacute;ron says, \"In Reinforcement Learning, a software *agent*\n",
    "makes *observations* and takes *actions* within an\n",
    "*environment*, and in return it receives *rewards*\" (pg 446).\n",
    "These terms are chosen to be general enough to be applicable in the real world\n",
    "as well as in virtual worlds. \n",
    "Taking them one at a time,\n",
    "\n",
    "- An *agent* is a software entity or the real-world entity (such as a robot\n",
    "or device) that the software controls.\n",
    "\n",
    "- *Observations* are the readings fed into the agent, the inputs the agent uses\n",
    "to determine the next action.\n",
    "They could, for example, come from sensors.\n",
    "\n",
    "- *Actions* are the agent's outputs, the things the agent does.\n",
    "\n",
    "- The *environment* is the real or virtual setting from which the observations are made\n",
    "and which the actions affect.\n",
    "\n",
    "- The *reward* is what we want to maximize.\n",
    "Rewards also can be negative, in which case we minimize the absolute value.\n",
    "\n",
    "The most important of term is *policy*, which is the algorithm used by the agent.\n",
    "A policy can be hard-coded---that is, devised by a human, as we usually\n",
    "think of programming; \n",
    "or it can be learned/trained.\n",
    "Training can be thought of as a search in the *policy space*, the set of \n",
    "possible policies.\n",
    "That search could be done by genetic algorithms, or \n",
    "using a *policy gradient*, similar to how we have used gradient descent\n",
    "in other techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The OpenAI Gym package\n",
    "\n",
    "To train an agent or policy, you need an environment for the agent to interact with.\n",
    "An environment can be part of the real world, but of course there are many advantages to\n",
    "simulated environments. \n",
    "(As G&eacute;ron says, \"If a robot falls off a cliff, you can't just click `undo`\" (pg 449).)\n",
    "We'll be using the `gym` package from the [OpenAI project](https://gym.openai.com/)\n",
    "which contains simulated environments and library functions to interact with them.\n",
    "\n",
    "First, import some packages and define a few functions our code will need later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "    \n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by using an environment called CartPole, which is... a simulated *cart*... with a *pole*.\n",
    "As described in the [OpenAI Gym documentation](https://gym.openai.com/envs/CartPole-v0/),\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "There also exists a Ms Pacman environment, which would have been more exciting. \n",
    "Unfortunately the `atari` dependency that it needs won't install correctly with our current version of Python 3.\n",
    "\n",
    "Actually the \"Cart Pole\" is a classic problem. \n",
    "Physcially it's an inverted pendulum, that is, it's a pendulum with a pivot on the bottom.\n",
    "Keeping the pole centered is difficult, but we're going to develop a policy to do it.\n",
    "\n",
    "You can see what the cart does with this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take this apart. Resetting the environment gives us\n",
    "an initial observation. For the Cart Pole environment, an observation\n",
    "consists in the horizontal position of the cart, the cart's velocity,\n",
    "the angle of the pole (in radians (I think) from vertical),\n",
    "and the angular velocity of the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code makes the visualization cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "try:\n",
    "    from pyglet.gl import gl_info\n",
    "    openai_cart_pole_rendering = True   # no problem, let's use OpenAI gym's rendering function\n",
    "except Exception:\n",
    "    openai_cart_pole_rendering = False  # probably no X server available, let's use our own rendering function\n",
    "\n",
    "def render_cart_pole(env, obs):\n",
    "    if openai_cart_pole_rendering:\n",
    "        # use OpenAI gym's rendering function\n",
    "        return env.render(mode=\"rgb_array\")\n",
    "    else:\n",
    "        # rendering for the cart pole environment (in case OpenAI gym can't do it)\n",
    "        img_w = 600\n",
    "        img_h = 400\n",
    "        cart_w = img_w // 12\n",
    "        cart_h = img_h // 15\n",
    "        pole_len = img_h // 3.5\n",
    "        pole_w = img_w // 80 + 1\n",
    "        x_width = 2\n",
    "        max_ang = 0.2\n",
    "        bg_col = (255, 255, 255)\n",
    "        cart_col = 0x000000 # Blue Green Red\n",
    "        pole_col = 0x669acc # Blue Green Red\n",
    "\n",
    "        pos, vel, ang, ang_vel = obs\n",
    "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        cart_x = pos * img_w // x_width + img_w // x_width\n",
    "        cart_y = img_h * 95 // 100\n",
    "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
    "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
    "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
    "        draw.rectangle((cart_x - cart_w // 2, cart_y - cart_h // 2, cart_x + cart_w // 2, cart_y + cart_h // 2), fill=cart_col) # draw cart\n",
    "        draw.line((cart_x, cart_y - cart_h // 2, top_pole_x, top_pole_y), fill=pole_col, width=pole_w) # draw pole\n",
    "        return np.array(img)\n",
    "\n",
    "def plot_cart_pole(env, obs):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    img = render_cart_pole(env, obs)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the state of the environment by rendering an image for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode=\"rgb_array\")\n",
    "#print(img)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actions are possible in this environment?\n",
    "That's defined by the environment's *action space*, which we can look at as an attribute of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there is a discrete (rather than continuous) set of actions, and specifically there\n",
    "are two of them.\n",
    "In Cart Pole, these two actions correspond to \"applying a force of +1 or -1 to the cart\", as described\n",
    "earlier. \n",
    "Let's try moving the pole to the right.\n",
    "This will return a new observation, as well as a reward, an indicator of whether we're done,\n",
    "and \"other\" info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs) # Print the original observation again for convenience\n",
    "action = 1\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the observation tell you about how the environment has changed?\n",
    "\n",
    "In this environment, the goal is to keep the pole from tipping over as long as possible.\n",
    "The `done` result will be `True` when the cart leaves the environment or the pole leans too much.\n",
    "Thus the `reward` is given merely for keeping the game alive---it will be 1.0 at every step \n",
    "as long as the episode isn't over.\n",
    "In this case the `info` is empty, but in general (according to G&eacute;ron, pg 452):\n",
    "\n",
    "> This dictionary may provide extra debug information in other environments.\n",
    "This data should not be used for training (it would be cheating).\n",
    "\n",
    "Let's look at it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually nothing much seems to have changed.\n",
    "This time, let's repeat an action until there are significant changes.\n",
    "This code keeps pushing the cart left, and stops when `done` is `True`, that is,\n",
    "when the pole tips over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, the ol' pole is tipped over now.\n",
    "Go back, reset the environment, and change the code so that it pushes the car to the right instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A hard-coded policy\n",
    "\n",
    "Now let's make a first attempt at a policy to solve the problem.\n",
    "Of course our real goal is to *train* a policy, but in principle we can hard code one\n",
    "of our own devising. \n",
    "The following encodes a policy of just pushing the cart in the same direction that the pole is currently \n",
    "(slightly) tilting.\n",
    "The intuition, I guess, is that we're trying to move the pole's pivot under its center of gravity.\n",
    "\n",
    "So that we can watch what happens, we store the rendered image of each step\n",
    "in a list, where they are like frames in an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(n_max_steps):\n",
    "    img = render_cart_pole(env, obs)\n",
    "    frames.append(img)\n",
    "\n",
    "    # hard-coded policy\n",
    "    position, velocity, angle, angular_velocity = obs\n",
    "    if angle < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch it. \n",
    "(I put this in a separate cell so you could watch the animation of the same run of the game several times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = plot_animation(frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't work very well.\n",
    "Make some changes with the hard-coded policy to see if you can make it stay up for longer. Also, you can comment out the `if done : break ` lines to see what happens if your policy keeps on being used even after the pole is tilted too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A neural net policy\n",
    "\n",
    "Now let's make a policy that uses a neural net to control the cart.\n",
    "The input units will be the observations, and the output will be the action.\n",
    "Read through this code (which uses `tensorflow`) and see how well you can figure \n",
    "out how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Specify the network architecture\n",
    "n_inputs = 4  # == env.observation_space.shape[0]\n",
    "n_hidden = 4  # it's a simple task, we don't need more than this\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
    "                         kernel_initializer=initializer)\n",
    "outputs = tf.layers.dense(hidden, n_outputs, activation=tf.nn.sigmoid,\n",
    "                          kernel_initializer=initializer)\n",
    "\n",
    "# 3. Select a random action based on the estimated probabilities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has one hidden layer with four units, and the output layer has one unit,\n",
    "since our output is moving the cart to the left or right.\n",
    "However, the output unit doesn't give the action exactly, but rather gives\n",
    "the probability of an action.\n",
    "Specifically, the output is a probability $p$ of moving the cart left \n",
    "(or $p-1$ of moving the car right).\n",
    "The `tf.multinomial` business in the code does random sampling of\n",
    "a multinomial distribution to determine and action based on the \n",
    "probability returned by the output unit.\n",
    "\n",
    "The reason for the randomness is training.\n",
    "The current state of the network/policy will indicate one action or the other\n",
    "is preferrable, but just in case that's *not* the right thing to do at this point,\n",
    "there is a chance that our agent will pick the other possible action.\n",
    "If that other action turns out to be better, then that observation can\n",
    "be used to train the policy better.\n",
    "The network \"explores\" new ways to respond to an observation.\n",
    "\n",
    "Run this policy on one round of the game, and then watch it.\n",
    "The weights of the network are set up randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_steps = 1000\n",
    "frames = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        img = render_cart_pole(env, obs)\n",
    "        frames.append(img)\n",
    "        action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "        obs, reward, done, info = env.step(action_val[0][0])\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = plot_animation(frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did this do? Since your network was set up randomly, I can't predict for\n",
    "certain, but I'm guessing not very well.\n",
    "You can try again.\n",
    "Remember that running the `plt.show()` cell multiply will replay the same video, but\n",
    "re-running the prior cell will make a new network and run a new round of the game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train the neural net. \n",
    "The following code uses parts of `tensorflow` that we didn't get to,\n",
    "but the idea is that we're adding a trainer and something to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_outputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits) # probability of action 0 (left)\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run and watch this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_steps = 1000\n",
    "frames = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        img = render_cart_pole(env, obs)\n",
    "        frames.append(img)\n",
    "        action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "        obs, reward, done, info = env.step(action_val[0][0])\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = plot_animation(frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That one run still relies on the network's random initialization.\n",
    "But if we run it many times, it will train.\n",
    "To speed things, up train it in 10 environments in parallel, each with 1000 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 10\n",
    "n_iterations = 1000\n",
    "\n",
    "envs = [gym.make(\"CartPole-v0\") for _ in range(n_environments)]\n",
    "observations = [env.reset() for env in envs]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        target_probas = np.array([([1.] if obs[2] < 0 else [0.]) for obs in observations]) # if angle<0 we want proba(left)=1., or else proba(left)=0.\n",
    "        action_val, _ = sess.run([action, training_op], feed_dict={X: np.array(observations), y: target_probas})\n",
    "        for env_index, env in enumerate(envs):\n",
    "            obs, reward, done, info = env.step(action_val[env_index][0])\n",
    "            observations[env_index] = obs if not done else env.reset()\n",
    "    saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy_net(model_path, action, X, n_max_steps = 1000):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    obs = env.reset()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "        for step in range(n_max_steps):\n",
    "            img = render_cart_pole(env, obs)\n",
    "            frames.append(img)\n",
    "            action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "            obs, reward, done, info = env.step(action_val[0][0])\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return frames        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(\"./my_policy_net_basic.ckpt\", action, X)\n",
    "video = plot_animation(frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net has learned the policy we had previous hand-coded: move the cart in the direction of the tilt.\n",
    "Of course, that's not a very good policy.\n",
    "Let's see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policy gradients\n",
    "\n",
    "*Policy gradient (PG) algorithms* train policies by following a gradient.\n",
    "As a policy is trained, we want good actions to increase in probability and\n",
    "bad actions to decrease.\n",
    "The difficulty is that we don't know whether a policy is good until several\n",
    "steps later when its long-term effects are seen. \n",
    "This is the *credit assignment problem*: \n",
    "If a series of actions result in a reward (or negaitive reward), which of the actions\n",
    "should be given credit for the reward and hence increased in probability?\n",
    "\n",
    "The *discount rate* is the amount of reward applied across steps to each action.\n",
    "For discount rate $r$ and a given reward $d$, $d$ is applied to the immediately preceding action, $r d$ to the action after that, $r^2 d$ to the action after that, etc.\n",
    "Alternately, if $d_i$ is the reward for step $i$, then the reward applied to\n",
    "the action at step $j$ is $\\sum_k r^k d_{j+k}$.\n",
    "\n",
    "*REINFORCE algorithms* are a class of PG algorithms, summarized by G&eacute;ron as follows:\n",
    "\n",
    "> 1. First, let the neural network policy play the game several times and at each step\n",
    "compute the gradients that would make the chosen action even more likely, but\n",
    "don't apply these gradients yet.\n",
    "2. Once you have run several episodes, compute each action's score.\n",
    "3. If an action's score is positive, it means that the action was good and you want\n",
    "to apply the gradients computed earlier to make the action even more likely to be chosen in the future. [The opposite for negative gradients.]\n",
    "4. Compute the mean of all the resulting gradient vectors, and use it to perform a Gradient Descent step. (Pg 457)\n",
    "\n",
    "Here is the code for this approach.\n",
    "One difference is that the optimizer uses `compute_gradients()` instead of\n",
    "`minimize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)  # probability of action 0 (left)\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "y = 1. - tf.to_float(action)\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to calculate discounts and apply rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try these to understand how those functions work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "n_games_per_update = 10\n",
    "n_max_steps = 1000\n",
    "n_iterations = 250\n",
    "save_iterations = 10\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be patient as those iterations tick by. When they are done, you should have\n",
    "a policy that can balance the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(\"./my_policy_net_pg.ckpt\", action, X, n_max_steps=1000)\n",
    "video = plot_animation(frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see more about Cart Pole, including other solutions, look \n",
    "[here](http://gym.openai.com/envs/CartPole-v0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
